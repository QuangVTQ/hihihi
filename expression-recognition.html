<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Simple Real-time Expression Detector</title>
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta http-equiv="Content-Security-Policy" content="default-src 'self' 'unsafe-inline' 'unsafe-eval' https://cdn.jsdelivr.net; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://cdn.jsdelivr.net; img-src 'self' data: blob:;" />
  <style>
    :root {
      --bg: #0f1724;
      --card: #0b1220;
      --accent: #7dd3fc;
      --muted: #94a3b8;
    }
    html,body { height:100%; margin:0; font-family:system-ui,Segoe UI,Roboto,Helvetica,Arial; background:linear-gradient(180deg,#071029 0%, #071a2b 100%); color:#e6eef6; display:flex; align-items:center; justify-content:center; }
    .card {
      width: 880px;
      max-width: calc(100% - 32px);
      background: rgba(255,255,255,0.02);
      border-radius:12px;
      padding:14px;      height:auto;
      box-shadow: 0 6px 30px rgba(2,6,23,0.7);
      display:flex;
      gap:12px;
      align-items:flex-start;
    }
    .preview {
      position:relative;
      width:640px;
      max-width:60vw;
      background:#000;
      border-radius:8px;
      overflow:hidden;
    }
    video { display:block; width:100%; height:auto; transform: scaleX(-1); } /* mirror */
    canvas { position:absolute; left:0; top:0; pointer-events:none; width:100%; height:100%; transform: scaleX(-1); }
    .sidebar {
      width:220px;
      min-width:180px;
      display:flex;
      flex-direction:column;
      gap:10px;
    }
    .status {
      font-size:13px;
      color:var(--muted);
    }
    .big {
      font-size:20px;
      font-weight:600;
      color:var(--accent);
    }
    .button {
      background:linear-gradient(180deg,#0ea5e9,#0284c7);
      color:#001219;
      padding:8px 10px;
      border-radius:8px;
      border:none;
      cursor:pointer;
      font-weight:600;
    }
    .small {
      font-size:12px;
      color:var(--muted);
    }
    .meter {
      height:10px;
      background:rgba(255,255,255,0.06);
      border-radius:999px;
      overflow:hidden;
    }
    .meter > i { display:block; height:100%; width:0%; background:linear-gradient(90deg,#7dd3fc,#60a5fa); }
    footer.small { margin-top:8px; color:#9fb6cf; font-size:12px; }
  </style>
</head>
<body>

  <div class="card">
    <div class="preview">
      <video id="video" autoplay muted playsinline></video>
      <canvas id="overlay"></canvas>      
    </div>

    <div class="sidebar">
      <div>
        <div class="status small">Model status</div>
        <div id="modelStatus" class="big">loading…</div>
      </div>

      <div>
        <div class="status small">Detected expression</div>
        <div id="exprLabel" class="big">—</div>
        <div id="confidence" class="small">confidence: —</div>
        <div class="meter" style="margin-top:8px">
          <i id="meterFill"></i>
        </div>
      </div>

      <div>
        <div class="status small">Controls</div>
        <button id="btnToggle" class="button">Start camera</button>
        <div class="small" style="margin-top:8px">Tip: open console for logs.</div>
      </div>

      <footer class="small">
        Simple HTML/CSS/JS demo — face-api.js
      </footer>
    </div>
  </div>

<script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
<script>
(async () => {
  const video = document.getElementById('video');
  const canvas = document.getElementById('overlay');
  const ctx = canvas.getContext('2d');
  const modelStatus = document.getElementById('modelStatus');
  const exprLabel = document.getElementById('exprLabel');
  const confidenceText = document.getElementById('confidence');
  const meterFill = document.getElementById('meterFill');
  const btnToggle = document.getElementById('btnToggle');

  let running = false;
  let speakCooldown = 0;

  // Where to load models from — put the model files in ./models
  // Models required: tiny_face_detector_model-shard1, tiny_face_detector_model-weights_manifest.json,
  // and face_expression_model-shard1, face_expression_model-weights_manifest.json
  // Download them from face-api.js model repo and place them in ./models folder
  const MODEL_URL = './models';

  async function loadModels() {
    try {
      // Wait for faceapi to be available
      await new Promise((resolve) => {
        if (window.faceapi) resolve();
        else {
          const checkInterval = setInterval(() => {
            if (window.faceapi) {
              clearInterval(checkInterval);
              resolve();
            }
          }, 100);
        }
      });

      modelStatus.textContent = 'loading models…';
      console.log('Loading models from:', MODEL_URL);
      
      // Load models one by one with verification
      modelStatus.textContent = 'loading face detector...';
      await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
      console.log('Face detector loaded');
      
      modelStatus.textContent = 'loading expression model...';
      await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
      console.log('Expression model loaded');
      
      modelStatus.textContent = 'loading landmarks...';
      await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
      console.log('Landmarks loaded');
      
      modelStatus.textContent = 'all models loaded successfully';
      console.log('All models loaded successfully');
    } catch (err) {
      console.error('Error loading models:', err);
      modelStatus.textContent = 'Error loading models: ' + err.message;
      throw err;
    }
  }

  async function startCamera() {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        video: { 
          facingMode: 'user',
          width: { ideal: 640 },
          height: { ideal: 480 }
        }, 
        audio: false 
      });
      
      video.srcObject = stream;
      video.width = 640;
      video.height = 480;
      
      // Wait for video to be ready
      await new Promise((resolve) => {
        video.onloadedmetadata = () => {
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          console.log('Video metadata loaded:', video.videoWidth, 'x', video.videoHeight);
          resolve();
        };
      });
      
      await video.play();
      running = true;
      btnToggle.textContent = 'Stop camera';
      runDetectionLoop();
    } catch (err) {
      console.error('Camera error', err);
      modelStatus.textContent = 'camera error — allow camera and reload';
    }
  }

  function stopCamera() {
    running = false;
    btnToggle.textContent = 'Start camera';
    const s = video.srcObject;
    if (s && s.getTracks) s.getTracks().forEach(t=>t.stop());
    video.srcObject = null;
    ctx.clearRect(0,0,canvas.width, canvas.height);
    exprLabel.textContent = '—';
    confidenceText.textContent = 'confidence: —';
    meterFill.style.width = '0%';
  }

  btnToggle.addEventListener('click', () => {
    if (!running) startCamera(); else stopCamera();
  });

  // mapping: choose the dominant expression and simple heuristic for "stressed"
  function findDominantExpression(expressions) {
    // expressions is an object: {neutral:0.3, happy:0.5, sad:0.02, angry:0.05, fearful:0.02, disgusted:0.01, surprised:0.1}
    let max = 0;
    let label = 'neutral';
    for (const k in expressions) {
      if (expressions[k] > max) { max = expressions[k]; label = k; }
    }
    return { label, confidence: max };
  }

  function isStressed(label, expressions) {
    // heuristic: high 'angry' or 'fearful' or 'sad' or 'disgusted' => stress
    const stressScore = (expressions.angry || 0) + (expressions.fearful || 0) + (expressions.sad || 0) + (expressions.disgusted || 0);
    return { stressed: stressScore > 0.4 || (label==='angry' || label==='fearful' || label==='sad'), stressScore };
  }

  function drawDetections(detection) {
    ctx.clearRect(0,0,canvas.width, canvas.height);
    if (!detection) return;
    // draw box (mirror matches video transform)
    ctx.lineWidth = 2;
    ctx.strokeStyle = 'rgba(125,211,252,0.9)';
    const box = detection.detection.box;
    ctx.strokeRect(box.x, box.y, box.width, box.height);
    // optional: draw landmarks if available
    if (detection.landmarks) {
      ctx.fillStyle = 'rgba(125,211,252,0.9)';
      const pts = detection.landmarks.positions;
      for (let p of pts) {
        ctx.beginPath();
        ctx.arc(p.x, p.y, 1.6, 0, Math.PI*2);
        ctx.fill();
      }
    }
  }

  async function runDetectionLoop() {
    // Use a more permissive score threshold to improve detection reliability
    const tinyFaceDetectorOptions = new faceapi.TinyFaceDetectorOptions({ inputSize: 416, scoreThreshold: 0.4 });

    while (running) {
      if (video.readyState < 2) {
        await new Promise(r => setTimeout(r, 100));
        continue;
      }

      try {
        // Perform all detections in a single call for efficiency
        const result = await faceapi.detectSingleFace(video, tinyFaceDetectorOptions)
          .withFaceLandmarks()
          .withFaceExpressions();

        if (result) {
          drawDetections(result);
          const dom = findDominantExpression(result.expressions);
          exprLabel.textContent = dom.label;
          confidenceText.textContent = `confidence: ${(dom.confidence * 100).toFixed(0)}%`;
          meterFill.style.width = `${Math.min(100, Math.round(dom.confidence * 100))}%`;

          const stress = isStressed(dom.label, result.expressions);
          if (stress.stressed) {
            exprLabel.textContent = `${dom.label} (stressed)`;
            // speak a short support message every ~8s
            const now = Date.now();
            if (now - speakCooldown > 8000) {
              speakCooldown = now;
              speakAdvice(dom.label);
            }
          }
        } else {
          // If no face is detected, clear the canvas and reset the UI
          ctx.clearRect(0, 0, canvas.width, canvas.height);
          exprLabel.textContent = 'no face';
          confidenceText.textContent = 'confidence: —';
          meterFill.style.width = '0%';
        }
      } catch (err) {
        console.error('Detection error:', err);
      }
        
        // small delay so CPU isn't hammered
        await new Promise(r => setTimeout(r, 180)); // ~5-6 fps
    }
  }

  function speakAdvice(label) {
    // simple supportive messages: change as you like
    const messages = {
      angry: "Take a slow breath. You're doing okay — try to relax your shoulders.",
      fearful: "You're looking worried. Breathe in for four, out for four.",
      sad: "It's okay to feel down. A short break or a glass of water might help.",
      disgusted: "Notice that feeling; stepping away for a minute can help.",
      neutral: "All good — you're looking calm.",
      happy: "Nice! Keep that smile.",
      surprised: "You look surprised — hope it's a good surprise!"
    };
    const text = messages[label] || "I notice some tension. Try a short breathing break.";
    if ('speechSynthesis' in window) {
      const u = new SpeechSynthesisUtterance(text);
      u.lang = 'en-US';
      window.speechSynthesis.cancel();
      window.speechSynthesis.speak(u);
    }
  }

  // init
  try {
    await loadModels();
    modelStatus.textContent = 'ready — click Start camera';
  } catch (e) {
    console.error('model load error', e);
    modelStatus.textContent = 'model load failed — put model files in /models';
  }
})();
</script>
</body>
</html>